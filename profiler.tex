\section{Profiling}
	\begin{itemize}
		\item Question: different data structures have different complexities. Adding something
			might be cheap on one, but extensive on another that is faster on removing them. But
			how can we compare them based on the accesses used?
		\item Idea: count each call to the data structures, compute the complexities at the
			end and give recommendations
		\item Achieved through AspectJ which can be used to add functionalities to existing code
			(see section \ref{sec:aspectj}). This enables us to keep nearly all profiler
			functionality seperated from the other code: the aspects to enable counting reside in
			\texttt{ProfilerAspects.aj}, the counterpart to count the accesses reside in
			the class \texttt{Profiler}
		\item On runtime, the profiler counts calls based on the performed action (add, remove,
			contains) and callee signature (metric name, update type,\ldots), such that we can
			determine the complexities for different use cases with one execution
		\item Combined complexities are computed based on the complexities for each action on
			each data structure, which are stored within each data structure class
		\item Accesses are counted based on a list of distinguishable access types (add,
			remove, contains) and the callee signature (eg. metric name, update type)
		\item Access counters are written to files split up into different use cases (for
			graph generation, batch generation and application, metrics), summarized
			for each callee, and a recommendation for optimized complexities. As the computation of
			recommendations is time consuming, this is not done for each callee, but only for
			aggregations.

			\begin{tabular}{@{\hspace{2ex}}p{42em}}	
			\dirtree{%
			.1 Root directory of data output.
			.2 aggr. 
			.2 run.0.
			.3 batch.0. 
			.4 metric.firstExample. 
			.5 \_\_\_metric.profiler.values.
			.4 metric.secondExample. 
			.5 \ldots .   
			.4 \_\_\_aggregated.profiler.values.
			.4 \_\_\_batchGeneration.profiler.values.
			.4 \_\_\_metric.profiler.values. 
			.4 \_\_\_updates.profiler.values.    
			.3 batch.1.
			.4 \ldots .    
			.3 \_\_\_aggregated.profiler.values. 
			.3 \_\_\_batchGeneration.profiler.values. 
			.3 \_\_\_graphGeneration.profiler.values. 
			.3 \_\_\_metric.profiler.values. 
			.3 \_\_\_updates.profiler.values.  
			.2 run.1. 
			.3 \ldots .   
			.2 \_\_\_aggregated.profiler.values. 
			.2 \_\_\_batchGeneration.profiler.values. 
			.2 \_\_\_metric.profiler.values. 
			.2 \_\_\_updates.profiler.values. 
			}
			\end{tabular}
		\item Example output (shortened, full file at \ref{sec:exampleOutput}):
			
		\begin{verbatim}
			DegreeDistributionU.AddNodeGlobal=0
			DegreeDistributionU.AddNodeLocal=0
			[...]
			DegreeDistributionU.SizeEdgeLocal=96000
			DegreeDistributionU.RandomNodeGlobal=0
			DegreeDistributionU.RandomEdgeGlobal=0
			[...]
			#  Aggr: 96000*O(1)
			#   Recommendations:
			#    DArray;DArray;DArray: 96000*O(1)
		\end{verbatim}
		
		\item Important side note about the recommender: different data structures use different
			ways to perform the same action (eg. \texttt{DArray.add} calls
			\texttt{DArray.contains}, while \texttt{DHashSet.add} does not). The profiler can only
			count each access type for the currently used data structures and give recommendations
			for this combination of calls. Thus, using another combination might not lead to the
			complexity the recommender computed. For the given example, the recommender will work
			based on calls for both \texttt{add} and \texttt{contains}, although \texttt{DHashSet}
			never calls \texttt{contains} within \texttt{add}. It might be necessary to run the
			recommender more than once to yield better results.
		\item It takes a nearly static amount of time to compute the recommendations,
			independent of the chosen data structures and the graph size. Having it active in a
			setup with a small graph predominates the optimization of data structures
	\end{itemize}
